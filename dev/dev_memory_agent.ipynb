{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent AC Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephbloom/GithubRepositories/DecisionTransformerInterpretability/src/ppo/agent.py:564: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mask = 1 - t.tensor(done, device=self.device, dtype=t.float)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch as t \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from src.models.trajectory_lstm import TrajectoryLSTM\n",
    "from src.ppo.agent import PPOAgent, PPOScheduler\n",
    "from src.ppo.memory import Memory\n",
    "from src.config import EnvironmentConfig, LSTMModelConfig, OnlineTrainConfig\n",
    "from src.ppo.utils import get_obs_shape \n",
    "from src.environments.environments import make_env\n",
    "from src.utils import DictList\n",
    "from src.ppo.agent import LSTMPPOAgent\n",
    "\n",
    "# now in order instantiate the class we need: the config files and the environment.\n",
    "\n",
    "environment_config = EnvironmentConfig()\n",
    "lstm_config = LSTMModelConfig(environment_config)\n",
    "online_config = OnlineTrainConfig()\n",
    "run_name = \"dev\"\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(\n",
    "        env_id=environment_config.env_id,\n",
    "        seed=environment_config.seed + i,\n",
    "        idx=i,\n",
    "        capture_video=environment_config.capture_video,\n",
    "        run_name=run_name,\n",
    "        max_steps=environment_config.max_steps,\n",
    "        fully_observed=environment_config.fully_observed,\n",
    "        flat_one_hot=environment_config.one_hot_obs,\n",
    "        agent_view_size=environment_config.view_size,\n",
    "        render_mode=\"rgb_array\",\n",
    "    ) for i in range(online_config.num_envs)]\n",
    ")\n",
    "\n",
    "lstm_agent = LSTMPPOAgent(envs=envs, environment_config=environment_config, lstm_config=lstm_config, device=t.device(\"cpu\"))\n",
    "memory = Memory(envs, online_config, device=t.device(\"cpu\"))\n",
    "lstm_agent.rollout(memory, online_config.num_steps, envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ppo.loss_functions import calc_clipped_surrogate_objective, calc_value_function_loss, calc_entropy_bonus\n",
    "import torch.nn as nn\n",
    "\n",
    "update_epochs = online_config.update_epochs\n",
    "args = online_config\n",
    "num_updates = online_config.total_timesteps // online_config.batch_size\n",
    "optimizer, scheduler = lstm_agent.make_optimizer(\n",
    "    num_updates,\n",
    "    online_config.learning_rate,\n",
    "    online_config.learning_rate * 1e-4\n",
    ")\n",
    "\n",
    "for _ in range(update_epochs):\n",
    "    minibatches = memory.get_minibatches()\n",
    "    # Compute loss on each minibatch, and step the optimizer\n",
    "    for mb in minibatches:\n",
    "        obs = lstm_agent.preprocess_obs(DictList(mb.obs))\n",
    "        results = lstm_agent.model(obs, mb.recurrence_memory) # shouldn't this be from the previous timestep?\n",
    "        probs = results['dist']\n",
    "        values = results['value']\n",
    "        recurrence_memory = results['memory']\n",
    "        clipped_surrogate_objective = calc_clipped_surrogate_objective(\n",
    "            probs, mb.actions, mb.advantages, mb.logprobs, args.clip_coef)\n",
    "        value_loss = calc_value_function_loss(\n",
    "            values, mb.returns, args.vf_coef)\n",
    "        entropy_bonus = calc_entropy_bonus(probs, args.ent_coef)\n",
    "        total_objective_function = clipped_surrogate_objective - value_loss + entropy_bonus\n",
    "        optimizer.zero_grad()\n",
    "        total_objective_function.backward()\n",
    "        nn.utils.clip_grad_norm_(lstm_agent.model.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "# Step the scheduler\n",
    "scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision_transformer_interpretability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0be95f1efa079bcf597630754731f3dc2b2137553763cb34bfd5652600bd2735"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
